{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Snowflake Read/Write with GDS, Spark Connector and PySpark\n",
    "In the examples that follows, we will be using the Spark Connector running under PySpark\n",
    "[Neo4j spark connector under Python](https://neo4j.com/docs/spark/current/python/) and the Graph Data Science (GDS) client.\n",
    "\n",
    "Please run this notebook from a valid Spark environment.  It was tested under [DataProc](https://cloud.google.com/dataproc)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Setup Neo4j instance\n",
    "Create a free account at https://sandbox.neo4j.com. Choose the “Blank Sandbox - Graph Data Science” option.\n",
    "When your sandbox has been created, fill in the Bolt URL and password below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pip install --upgrade seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pip install --upgrade matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Setup Neo4j Spark Connector imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from graphdatascience import GraphDataScience\n",
    "from pyspark.sql import SparkSession\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "from pyspark import SparkFiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Define Neo4j connection variables.  Yours will be different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "neo4j_url = \"bolt://***removed***:7687\"\n",
    "neo4j_user = \"neo4j\"\n",
    "neo4j_password = \"***removed***\"\n",
    "neo4j_database= \"neo4j\"\n",
    "tmp_working_bucket = \"neo4j-sandbox/dataproc-working\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Snowflake\n",
    "snowflake_uri =  \"https://***removed***.snowflakecomputing.com\"\n",
    "snowflake_user =  \"***removed***\"\n",
    "snowflake_pwd =  \"***removed***\"\n",
    "snowflake_db =  \"***removed***\"\n",
    "snowflake_schema = \"CENSUS\"\n",
    "\n",
    "sfOptions={\n",
    "    \"sfURL\": snowflake_uri,\n",
    "    \"sfUser\": snowflake_user,\n",
    "    \"sfPassword\": snowflake_pwd,\n",
    "    \"sfDatabase\":snowflake_db,\n",
    "    \"sfSchema\": snowflake_schema\n",
    "}\n",
    "#sfWarehouse\n",
    "SNOWFLAKE_SOURCE_NAME = \"net.snowflake.spark.snowflake\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Create Spark Session, seeded with Neo4j packages.  If you don't want to wait for the download each time, load the connector into the master node using SSH.\n",
    "\n",
    "Note: we are adding the Neo4j data warehouse connector and [Snowflake library](https://search.maven.org/artifact/net.snowflake/spark-snowflake_2.12) in the library packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# spark and jdbc libraries must be aligned with Spark version 2.4\n",
    "# org.neo4j:neo4j-connector-apache-spark_2.12:4.1.3_for_spark_2.4,net.snowflake:spark-snowflake_2.12:2.9.3-spark_2.4,net.snowflake:snowflake-jdbc:3.13.14\n",
    "\n",
    "spark = (SparkSession.builder\n",
    "        .appName('Leverage Neo4j with Apache Spark')\n",
    "        .master('local[*]')\n",
    "        # Just to show dataframes as tables\n",
    "        .config('spark.sql.repl.eagerEval.enabled', True)\n",
    "        # On DataProc we must use spark 2.x\n",
    "        .config('spark.jars.packages', \"org.neo4j:neo4j-connector-apache-spark_2.12:4.1.3_for_spark_2.4,net.snowflake:spark-snowflake_2.12:2.9.3-spark_2.4,net.snowflake:snowflake-jdbc:3.13.14\")\n",
    "        # These global credentials don't cascade on DataProc.  Provided here for use in the future.\n",
    "        .config('neo4j.url', neo4j_url)\n",
    "        .config('neo4j.authentication.type', \"basic\")\n",
    "        .config('neo4j.authentication.basic.username', neo4j_user)\n",
    "        .config('neo4j.authentication.basic.password', neo4j_password)\n",
    "        .getOrCreate())\n",
    "# output spark version\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's create a handle to the Graph Data Science library so we can make GDS calls and scripts more elegantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "gds = GraphDataScience(neo4j_url, auth=(neo4j_user, neo4j_password))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's now check that GDS is running on the server by executing this Cypher query.\n",
    "We only need to supply credentials once per notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "gds.run_cypher(\"return gds.version() as gds_version\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Query to see what data is there already using Spark Connector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "spark.read.format(\"org.neo4j.spark.DataSource\") \\\n",
    "    .option(\"url\", neo4j_url) \\\n",
    "    .option('authentication.type', \"basic\") \\\n",
    "    .option('authentication.basic.username', neo4j_user) \\\n",
    "    .option('authentication.basic.password', neo4j_password) \\\n",
    "    .option(\"labels\", \"MSA\").load().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "If dataset is not empty, reset it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "gds.run_cypher(\"CREATE OR REPLACE DATABASE `\"+neo4j_database+\"`\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Check that it's empty now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "gds.run_cypher(\"MATCH (n:MSA) RETURN count(n)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Load MSA data from Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "First, let's try downloading a file from URL the Spark way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/smithna/datasets/main/CensusDemographicsByMetroArea.csv\"\n",
    "spark.sparkContext.addFile(url)\n",
    "urlDf = spark.read.csv(\"file://\"+SparkFiles.get(\"CensusDemographicsByMetroArea.csv\"), header=True, inferSchema= True)\n",
    "urlDf.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Check to see what kind of job Spark did inferring schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "urlDf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We see that percentOver25WithBachelors is a double, which is not supported in Neo4j 4.x\n",
    "We can cast to float using selectExpr."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "floatCastSelectExpr=[\"NAME as name\",\"population\",\\\n",
    "\"cast(PERCENTOVER25WITHBACHELORS as float) percentOver25WithBachelors\",\\\n",
    "\"cast(MEDIANHOMEPRICE as int) medianHomePrice\",\\\n",
    "\"cast(MEDIANHOUSEHOLDINCOME as int) medianHouseholdIncome\"]\n",
    "\n",
    "castUrlDf = urlDf.selectExpr(floatCastSelectExpr)\n",
    "castUrlDf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now delete it, we are loading now from BigQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "gds.run_cypher(\"CREATE OR REPLACE DATABASE `\"+neo4j_database+\"`\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "gds.run_cypher(\"MATCH (n:MSA) RETURN count(n)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Load MSA data from Snowflake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "This section leverages best practices described here: https://neo4j.com/docs/spark/current/dwh/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Create unique constraint on MSA.  The column \"name\" is the msa name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "gds.run_cypher(\"\"\"\n",
    "CREATE CONSTRAINT msa_name IF NOT EXISTS ON (m:MSA) ASSERT m.name IS NODE KEY\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Query Snowflake table\n",
    "\n",
    "https://docs.snowflake.com/en/user-guide/spark-connector-use.html\n",
    "https://github.com/snowflakedb/spark-snowflake/releases/tag/v2.9.3-spark_3.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's check out sfOptions and make sure it's populated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "SNOWFLAKE_SOURCE_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sfOptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# If we're doing a table scan, the options table syntax can be used and it provides multi-threading\n",
    "# Neo4j does not support decimal types so we need to coerce doubles and there is no way to cast floats in a Snowflake query\n",
    "# So we will need to cast the percentOver25WithBachelors within the dataframe or on the insert side (requires unwind)\n",
    "# Spark to Neo4j casting guidelines here -- https://neo4j.com/docs/spark/current/types/\n",
    "msa_query=\"\"\"\n",
    "    SELECT NAME\n",
    "    ,POPULATION\n",
    "    ,PERCENT_OVER25_WITH_BACHELORS as percentOver25WithBachelors\n",
    "    ,MEDIAN_HOME_PRICE as medianHomePrice\n",
    "    ,MEDIAN_HOUSEHOLD_INCOME as medianHouseholdIncome\n",
    "    FROM CENSUS.MSA_DEMOS\n",
    "    WHERE name like '%Metro%'\n",
    "\"\"\"\n",
    "tableSnowflakeDf = (spark.read.format(SNOWFLAKE_SOURCE_NAME)\n",
    "                    .options(**sfOptions)\n",
    "                    .option(\"query\", msa_query)\n",
    "                    .load())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Print results schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tableSnowflakeDf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "When you try to load this dataframe into Neo4j you will get an error like this one:\n",
    "\n",
    "Unable to convert org.apache.spark.sql.types.Decimal to Neo4j Value\n",
    "Try it to demonstrate the point that Neo4j does not support doubles or decimals! They need to be down-cast to floats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "(tableSnowflakeDf.write\n",
    "  .format(\"org.neo4j.spark.DataSource\")\n",
    "  .mode(\"ErrorIfExists\")\n",
    "  .option(\"labels\", \":MSA\")\n",
    "  .option(\"node.keys\",\"name\")\n",
    "  .option(\"url\", neo4j_url) \n",
    "  .option(\"authentication.type\", \"basic\") \n",
    "  .option(\"authentication.basic.username\", neo4j_user) \n",
    "  .option(\"authentication.basic.password\", neo4j_password) \n",
    "  .option(\"partitions\", \"1\") \n",
    "  .save())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's first work-around by changing data typing in the data frame from decimal to float. There are a variety of methods, selectExpr is shown below. https://sparkbyexamples.com/spark/spark-change-dataframe-column-type/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "floatCastSelectExpr=[\"NAME as name\",\"cast(POPULATION as long) population\",\\\n",
    "\"cast(PERCENTOVER25WITHBACHELORS as float) percentOver25WithBachelors\",\\\n",
    "\"cast(MEDIANHOMEPRICE as int) medianHomePrice\",\\\n",
    "\"cast(MEDIANHOUSEHOLDINCOME as int) medianHouseholdIncome\"]\n",
    "\n",
    "\n",
    "castTableSnowflakeDf = tableSnowflakeDf.selectExpr(floatCastSelectExpr)\n",
    "castTableSnowflakeDf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now, write should work.  If this query fails because data already exists, you can change the mode or re-run database delete function above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "(castTableSnowflakeDf.write\n",
    "  .format(\"org.neo4j.spark.DataSource\")\n",
    "  .mode(\"ErrorIfExists\")\n",
    "  .option(\"labels\", \":MSA\")\n",
    "  .option(\"url\", neo4j_url) \n",
    "  .option(\"authentication.type\", \"basic\") \n",
    "  .option(\"authentication.basic.username\", neo4j_user) \n",
    "  .option(\"authentication.basic.password\", neo4j_password) \n",
    "  .option(\"partitions\", \"1\") \n",
    "  .save())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The Neo4j Spark Connector makes it easy to load data from a DataFrame into Neo4j."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Check for data now.  There should be 392 rows since we filtered out non-Metros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "gds.run_cypher(\"MATCH (n:MSA) RETURN count(n)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Read from Neo4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "neo4jDf=spark.read.format(\"org.neo4j.spark.DataSource\") \\\n",
    "  .option(\"labels\", \"MSA\") \\\n",
    "  .option(\"url\", neo4j_url) \\\n",
    "  .option(\"authentication.type\", \"basic\")  \\\n",
    "  .option(\"authentication.basic.username\", neo4j_user)  \\\n",
    "  .option(\"authentication.basic.password\", neo4j_password)  \\\n",
    "  .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "neo4jDf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Cast double to long.  Renaming could be done in the same step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cast_sql=\"\"\"\n",
    "    select name, \n",
    "    cast(POPULATION as long) population,\n",
    "    cast(PERCENTOVER25WITHBACHELORS as float) percentOver25WithBachelors,\n",
    "    cast(MEDIANHOMEPRICE as int) medianHomePrice,\n",
    "    cast(MEDIANHOUSEHOLDINCOME as int) medianHouseholdIncome\n",
    "    from msa_df\n",
    "\"\"\"\n",
    "neo4jDf.createOrReplaceTempView(\"msa_df\")\n",
    "neo4jDfCasted = spark.sql(cast_sql)\n",
    "neo4jDfCasted.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Convert Spark dataframe to pandas to display histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pandas_msa_df=neo4jDfCasted.toPandas()\n",
    "print(pandas_msa_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now show histograms of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(4, 2)\n",
    "fig.set_size_inches(15,30)\n",
    "for i in range(1,5):\n",
    "    sns.histplot(pandas_msa_df.iloc[:,i], ax=axes[i-1,0])\n",
    "    sns.histplot(pandas_msa_df.iloc[:,i], log_scale=True, ax=axes[i-1,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "That log transformation looks like it will help. Run the Cypher to store the transformed values in the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "log_new_view_sql = \"\"\"\n",
    "SELECT name, \n",
    "population, CAST(log(population)  as float) as logPopulation,\n",
    "medianHouseholdIncome,  CAST(log(medianHouseholdIncome) as float) as logMedianHouseholdIncome,\n",
    "medianHomePrice, CAST(log(medianHomePrice) as float) as logMedianHomePrice,\n",
    "CAST(percentOver25WithBachelors as float) as percentOver25WithBachelors, CAST(log(percentOver25WithBachelors) as float) as logPercentOver25WithBachelors\n",
    "FROM msa_df\n",
    "\"\"\"\n",
    "neo4jDfCasted.createOrReplaceTempView(\"msa_df\")\n",
    "neo4jDfLogUpdated = spark.sql(log_new_view_sql)\n",
    "neo4jDfLogUpdated.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Commit to the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# This write is Spark native\n",
    "(neo4jDfLogUpdated.write\n",
    "  .format(\"org.neo4j.spark.DataSource\")\n",
    "  .mode(\"Overwrite\")\n",
    "  .option(\"labels\", \":MSA\")\n",
    "  .option(\"node.keys\",\"name\")\n",
    "  .option(\"url\", neo4j_url) \n",
    "  .option(\"authentication.type\", \"basic\") \n",
    "  .option(\"authentication.basic.username\", neo4j_user) \n",
    "  .option(\"authentication.basic.password\", neo4j_password) \n",
    "  .option(\"partitions\", \"1\") \n",
    "  .save())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Look in the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "neo4jDf=spark.read.format(\"org.neo4j.spark.DataSource\") \\\n",
    "  .option(\"labels\", \"MSA\") \\\n",
    "  .option(\"url\", neo4j_url) \\\n",
    "  .option(\"authentication.type\", \"basic\")  \\\n",
    "  .option(\"authentication.basic.username\", neo4j_user)  \\\n",
    "  .option(\"authentication.basic.password\", neo4j_password)  \\\n",
    "  .load().printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Check that logs were committed to the database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Create in-memory graph projection\n",
    "Passing `\"*\"` as the third argument to `gds.graph.project` tells GDS to include any relationships that exist in the database in the in-memory graph. Because no relationships have been created in the graph yet, there will be no relationships in the in-memory graph projection when it is created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "g_msa, result = gds.graph.project(\n",
    "    'msa-graph', \n",
    "    'MSA', \n",
    "    '*', \n",
    "    nodeProperties = [\n",
    "        \"logPopulation\", \n",
    "        \"logMedianHouseholdIncome\", \n",
    "        \"logMedianHomePrice\", \n",
    "        \"logPercentOver25WithBachelors\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Notice that when we look at the results of gds.graph.project, we see that the relationshipCount is 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Apply MinMax scalar to property values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "gds.alpha.scaleProperties.mutate(g_msa, \n",
    "                                 nodeProperties = [\n",
    "                                     \"logPopulation\", \n",
    "                                     \"logMedianHouseholdIncome\", \n",
    "                                     \"logMedianHomePrice\", \n",
    "                                     \"logPercentOver25WithBachelors\"], \n",
    "                                 scaler = \"MinMax\",\n",
    "                                 mutateProperty = \"scaledProperties\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "This next line streams node properties to the procedure caller."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sp = gds.graph.streamNodeProperty(g_msa, \"scaledProperties\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Cleanup resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(list(sp['propertyValue'])).iloc[:,0].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(list(sp['propertyValue'])).iloc[:,1].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(list(sp['propertyValue'])).iloc[:,2].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(list(sp['propertyValue'])).iloc[:,3].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Run KNN to create relationships to nearest neighbors\n",
    "First run in stats mode and look at the similarity distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "knn_stats = gds.knn.stats(g_msa,\n",
    "                          nodeProperties={\"scaledProperties\":\"EUCLIDEAN\"},\n",
    "                          topK=15\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "knn_stats['similarityDistribution']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now run KNN in mutate mode to update the in-memory graph projection. We'll exclude the bottom 1% of similarity relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "gds.knn.mutate(g_msa,\n",
    "               nodeProperties={\"scaledProperties\":\"EUCLIDEAN\"},\n",
    "               topK=15,\n",
    "               mutateRelationshipType=\"IS_SIMILAR\",\n",
    "               mutateProperty=\"similarity\",\n",
    "               similarityCutoff=knn_stats['similarityDistribution']['p1']\n",
    "              )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Also write the relationships from the in-memory graph projection back to the on-disk graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "gds.graph.writeRelationship(\n",
    "    g_msa,\n",
    "    \"IS_SIMILAR\",\n",
    "    relationship_property=\"similarity\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Add a `rank` property to the `IS_SIMILAR` relationships for use with Bloom filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "gds.run_cypher(\"\"\"\n",
    "MATCH (m:MSA)-[s:IS_SIMILAR]->()\n",
    "WITH m, s ORDER BY s.similarity DESC\n",
    "WITH m, collect(s) as similarities, range(0, 19) AS ranks\n",
    "UNWIND ranks AS rank\n",
    "WITH rank, similarities[rank] AS rel\n",
    "SET rel.rank = rank + 1\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Run Louvain Community Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "See how many communities Louvain is going to recommend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "gds.louvain.stats(g_msa,\n",
    "                  relationshipTypes=[\"IS_SIMILAR\"],\n",
    "                 relationshipWeightProperty=\"similarity\"\n",
    "                 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now commit louvain communities to database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "gds.louvain.write(g_msa,\n",
    "                  relationshipTypes=[\"IS_SIMILAR\"],\n",
    "                  relationshipWeightProperty =\"similarity\",\n",
    "                  writeProperty=\"communityId\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Gather statistics about the communities that were discovered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Get average values for each community and 3 example MSAs for each community."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "communityDf = gds.run_cypher(\"\"\"\n",
    "MATCH (m:MSA)\n",
    "WITH m \n",
    "ORDER BY apoc.coll.sum([(m)-[s:IS_SIMILAR]->(m2) \n",
    "WHERE m.communityId = m2.communityId | s.similarity]) desc\n",
    "RETURN m.communityId as communityId,\n",
    "count(m) as msaCount, \n",
    "avg(m.population) as avgPopulation,\n",
    "avg(m.medianHomePrice) as avgHomePrice,\n",
    "avg(m.medianHouseholdIncome) as avgIncome,\n",
    "avg(m.percentOver25WithBachelors) as avgPctBachelors,\n",
    "collect(m.name)[..3] as exampleMSAs\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "communityDf.sort_values('communityId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(5, 1)\n",
    "fig.set_size_inches(6,20)\n",
    "for i in range(1,6):\n",
    "    sns.barplot(data=communityDf, x=\"communityId\", y=communityDf.columns[i], ax=axes[i-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Mean can give us a quick overview of properties, but can be skewed by outliers. Compare emperical cumulative distribution function (ECDF) at various proportions to get a more complete picture of distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "detailDf = gds.run_cypher(\"\"\"\n",
    "MATCH (m:MSA)\n",
    "RETURN \"community \" + m.communityId as communityId,\n",
    "m.population as population,\n",
    "m.medianHomePrice as medianHomePrice,\n",
    "m.medianHouseholdIncome as medianIncome,\n",
    "m.percentOver25WithBachelors as pctBachelors\n",
    "order by m.communityId\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(4, 1)\n",
    "fig.set_size_inches(6,20)\n",
    "for i in range(1,5):\n",
    "    sns.ecdfplot(data=detailDf, hue=\"communityId\", x=detailDf.columns[i], log_scale=True, ax=axes[i-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Compare two-dimensions on scatter plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "splot = sns.scatterplot(data=detailDf, x=\"medianIncome\", y=\"population\", hue=\"communityId\")\n",
    "splot.set(yscale=\"log\")\n",
    "splot.set(xscale=\"log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "splot = sns.scatterplot(data=detailDf, x=\"pctBachelors\", y=\"medianHomePrice\", hue=\"communityId\")\n",
    "splot.set(yscale=\"log\")\n",
    "splot.set(xscale=\"log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Write enriched features back to Snowflake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In this section, we are going to write features back to Snowflake.  Let's begin by previewing a query that we will materialize to Snowflake.  Note that you cannot use SKIP and LIMIT in cypher when using the Spark connector since it is batching the query internally.  You can use a flag, option.count, which was not working at time of publishing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "relationship_cypher=\"\"\"\n",
    "MATCH (s)-[r:IS_SIMILAR]-(t) \n",
    "RETURN s.name AS msa_name\n",
    ",t.name AS related_name\n",
    ",r.similarity AS similarity\n",
    ",s.communityId AS louvain_community_id\n",
    "\"\"\"\n",
    "relsPandaDf=gds.run_cypher(relationship_cypher)\n",
    "relsPandaDf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Convert pandas dataframe to Spark dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Create PySpark DataFrame from Pandas\n",
    "relsDf=spark.createDataFrame(relsPandaDf) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now write to bigquery.  Note that writes require a temporary storage location for Avro files in process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "(relsDf.write.format(SNOWFLAKE_SOURCE_NAME)\n",
    "    .options(**sfOptions)\n",
    "    .mode(\"overwrite\") \n",
    "    .option(\"dbtable\",\"msa_demo_similarity\")\n",
    "    .save())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now check to see that similarity records have been committed.  Expect around 11,000 records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "msa_query=\"\"\"\n",
    "    SELECT COUNT(*) NUM_SIMILARITY_RECORDS FROM MSA_DEMO_SIMILARITY\n",
    "\"\"\"\n",
    "(spark.read.format(SNOWFLAKE_SOURCE_NAME)\n",
    "                    .options(**sfOptions)\n",
    "                    .option(\"query\", msa_query)\n",
    "                    .load()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Optional: assign human-friendly names to the clusters discovered.\n",
    "The Louvain community detection algorithm is not deterministic. You should have roughly the same clusters from previous runs, but some edge cases might be assigned to different communities. The community numbers might be shuffled between across different runs.  \n",
    "**This step requires adjustment by hand: choose from community IDs above.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "gds.run_cypher(\"\"\"\n",
    "MATCH (m:MSA) \n",
    "  SET m.communityName = CASE m.communityId \n",
    "  WHEN 54 THEN \"Large mid-cost metros\"\n",
    "  WHEN 75 THEN \"College towns\"\n",
    "  WHEN 81 THEN \"Large high-cost metros\"\n",
    "  WHEN 234 THEN \"Mid-size metros\"\n",
    "  WHEN 264 THEN \"Small metros\"\n",
    "  WHEN 330 THEN \"Mid-price metros\"\n",
    "  WHEN 385 THEN \"Low-income metros\"\n",
    "  END\n",
    "return m.communityName, m.communityId, count(*)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Create an index on the communityName property to make it searchable in Bloom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "gds.run_cypher(\"\"\"\n",
    "CREATE INDEX msa_community_name IF NOT EXISTS\n",
    "FOR (m:MSA)\n",
    "ON (m.communityName)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now open Bloom and do some additional analysis!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Drop graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "graph_project_drop = \"\"\"\n",
    "    CALL gds.graph.drop(\n",
    "    'msa-graph')\n",
    "\"\"\"\n",
    "gds.run_cypher(graph_project_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "There is no Spark facility for deleting tables.   Snowflake publishes a Scala function \"Utils.runQuery\" but it is not  the Spark Standerd API and cannot be executed directly.  From PySpark, the method can be found here: spark._jvm.net.snowflake.spark.snowflake.Utils.runQuery\n",
    "https://docs.snowflake.com/en/user-guide/spark-connector-use.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "msa_drop_similarity_table_query=\"\"\"\n",
    "    DROP TABLE MSA_DEMO_SIMILARITY\n",
    "\"\"\"\n",
    "spark._jvm.net.snowflake.spark.snowflake.Utils.runQuery(sfOptions, msa_drop_similarity_table_query)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We can verify deletion here...  Error is expected!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "(spark.read.format(SNOWFLAKE_SOURCE_NAME)\n",
    "                    .options(**sfOptions)\n",
    "                    .option(\"query\", \"SELECT * FROM census.msa_demo_similarity\")\n",
    "                    .load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}