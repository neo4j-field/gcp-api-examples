{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Google Storage Read CSV with Neo4j Spark Connector and PySpark\n",
    "\n",
    "In the examples that follows, we will be using the Spark Connector running under PySpark\n",
    "[Neo4j spark connector under Python](https://neo4j.com/docs/spark/current/python/)\n",
    "Please run this notebook from a valid Spark environment.  It was tested under [DataProc](https://cloud.google.com/dataproc)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Setup Neo4j instance\n",
    "Create a free account at https://sandbox.neo4j.com. Choose the “Blank Sandbox - Graph Data Science” option.\n",
    "When your sandbox has been created, fill in the Bolt URL and password below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pip install --upgrade seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pip install --upgrade matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Setup Neo4j Spark Connector imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Define Neo4j connection variables.  Yours will be different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "neo4j_url = \"bolt://<ip>:7687\"\n",
    "neo4j_user = \"neo4j\"\n",
    "neo4j_password = \"<pwd>\"\n",
    "neo4j_database= \"neo4j\"\n",
    "tmp_working_bucket = \"neo4j-sandbox/dataproc-working\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Create Spark Session, seeded with Neo4j packages.  If you don't want to wait for the download each time, load the connector into the master node using SSH."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "spark = (SparkSession.builder\n",
    "        .appName('Leverage Neo4j with Apache Spark')\n",
    "        .master('local[*]')\n",
    "        # Just to show dataframes as tables\n",
    "        .config('spark.sql.repl.eagerEval.enabled', True)\n",
    "        # On DataProc we must use spark 2.x\n",
    "        .config('spark.jars.packages', 'org.neo4j:neo4j-connector-apache-spark_2.12:4.1.3_for_spark_2.4')       \n",
    "        .config(\"neo4j.url\", neo4j_url)\n",
    "        .config(\"neo4j.authentication.type\", \"basic\")\n",
    "        .config(\"neo4j.authentication.basic.username\", neo4j_user)\n",
    "        .config(\"neo4j.authentication.basic.password\", neo4j_password)\n",
    "        .getOrCreate())\n",
    "# output spark version\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Here we are going to create a utility class for the Spark Connector.  On DataProc the global configuration is not picked up so we need to supply credentials every time (clunk).  Also, some valid cypher statements will not run in the Spark Connector, as cypher.  They must be executed as side effects through the \"script\" argument of a dummy write operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class SparkConnector:\n",
    "    \n",
    "    def __init__( self, spark_session, uri, user, pwd):\n",
    "        \n",
    "        self.__spark_session = spark_session\n",
    "        self.__uri = uri\n",
    "        self.__user = user\n",
    "        self.__pwd = pwd\n",
    "        self.__count_cypher = \"MATCH (n) RETURN count(n)\"\n",
    "        self.__no_side_effects = \"MATCH (n) WHERE 1 = 2 SET n.placeholder = false\"\n",
    "        self.__df = self.read_cypher(self.__count_cypher)\n",
    "            \n",
    "    def read_cypher(self, query):\n",
    "        \n",
    "        response = None\n",
    "        try: \n",
    "            response = self.__spark_session.read.format(\"org.neo4j.spark.DataSource\") \\\n",
    "                .option(\"query\", query) \\\n",
    "                .option(\"url\", self.__uri) \\\n",
    "                .option(\"authentication.type\", \"basic\") \\\n",
    "                .option(\"authentication.basic.username\", self.__user) \\\n",
    "                .option(\"authentication.basic.password\", self.__pwd) \\\n",
    "                .option(\"partitions\", \"1\") \\\n",
    "                .load()\n",
    "        except Exception as e:\n",
    "            print(\"Read query failed:\", e)\n",
    "\n",
    "        return response\n",
    "    \n",
    "    def write_cypher(self, query):\n",
    "        \n",
    "        response = None\n",
    "        try: \n",
    "            response = self.__df.write.format(\"org.neo4j.spark.DataSource\") \\\n",
    "                .option(\"query\", query) \\\n",
    "                .option(\"url\", self.__uri) \\\n",
    "                .option(\"authentication.type\", \"basic\") \\\n",
    "                .option(\"authentication.basic.username\", self.__user) \\\n",
    "                .option(\"authentication.basic.password\", self.__pwd) \\\n",
    "                .option(\"partitions\", \"1\") \\\n",
    "                .save()\n",
    "        except Exception as e:\n",
    "            print(\"Write query failed:\", e)\n",
    "\n",
    "        return response\n",
    "    \n",
    "    def write_script(self,cypher):\n",
    "        \n",
    "        response = None\n",
    "        try: \n",
    "            response = self.__df.write.format(\"org.neo4j.spark.DataSource\") \\\n",
    "              .option(\"url\", self.__uri) \\\n",
    "              .option(\"authentication.type\", \"basic\") \\\n",
    "              .option(\"authentication.basic.username\", self.__user) \\\n",
    "              .option(\"authentication.basic.password\", self.__pwd) \\\n",
    "              .option(\"query\", self.__no_side_effects) \\\n",
    "              .option(\"script\",cypher) \\\n",
    "              .option(\"partitions\", \"1\") \\\n",
    "              .save()\n",
    "        except Exception as e:\n",
    "            print(\"Write query failed:\", e)\n",
    "\n",
    "        return response\n",
    "     \n",
    "\n",
    "             "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's now check that GDS is running on the server by executing this Cypher query.\n",
    "We only need to supply credentials once per notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sparkConnector = SparkConnector(spark_session=spark,uri=neo4j_url, user=neo4j_user, pwd=neo4j_password)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df=sparkConnector.read_cypher(\"return gds.version() as gds_version\")\n",
    "df.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sparkConnector.read_cypher(\"MATCH (n:MSA) RETURN count(n)\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Optional if database is not empty!  Reset it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "reset_db_query = \"\"\"CREATE OR REPLACE DATABASE `neo4j`\"\"\"\n",
    "sparkConnector.write_script(reset_db_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Check that it's empty now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sparkConnector.read_cypher(\"MATCH (n:MSA) RETURN count(n)\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Load MSA data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Create unique constraint on MSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sparkConnector.write_script(\"\"\"\n",
    "CREATE CONSTRAINT msa_name IF NOT EXISTS ON (m:MSA) ASSERT m.name IS NODE KEY\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Load CSV data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "load_csv_query = \"\"\"\n",
    "LOAD CSV WITH HEADERS FROM 'https://raw.githubusercontent.com/smithna/datasets/main/CensusDemographicsByMetroArea.csv' \n",
    "AS row\n",
    "WITH row WHERE row.name CONTAINS 'Metro'\n",
    "MERGE (m:MSA {name:row.name})\n",
    "SET m.population = toInteger(row.population),\n",
    "m.medianHouseholdIncome = toInteger(row.medianHouseholdIncome),\n",
    "m.medianHomePrice = toInteger(row.medianHomePrice),\n",
    "m.percentOver25WithBachelors = toFloat(row.percentOver25WithBachelors)\n",
    "RETURN count(m) as msaCount\"\"\"\n",
    "\n",
    "sparkConnector.write_script(load_csv_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "msa_df_query = \"\"\"\n",
    "MATCH (m:MSA)\n",
    "RETURN m.name AS msa, \n",
    "m.population AS population,\n",
    "m.medianHouseholdIncome AS medianHouseholdIncome,\n",
    "m.medianHomePrice AS medianHomePrice,\n",
    "m.percentOver25WithBachelors as percentOver25WithBachelors\"\"\"\n",
    "\n",
    "msa_df=sparkConnector.read_cypher(msa_df_query)\n",
    "msa_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Convert Spark dataframe to pandas to display histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pandas_msa_df=msa_df.toPandas()\n",
    "print(pandas_msa_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(4, 2)\n",
    "fig.set_size_inches(15,30)\n",
    "for i in range(1,5):\n",
    "    sns.histplot(pandas_msa_df.iloc[:,i], ax=axes[i-1,0])\n",
    "    sns.histplot(pandas_msa_df.iloc[:, i], log_scale=True, ax=axes[i-1,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "That log transformation looks like it will help. Run the Cypher to store the transformed values in the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "msa_df_update_query = \"\"\"\n",
    "MATCH (m:MSA)\n",
    "SET \n",
    "m.logPopulation = log(m.population),\n",
    "m.logMedianHouseholdIncome = log(m.medianHouseholdIncome),\n",
    "m.logMedianHomePrice = log(m.medianHomePrice),\n",
    "m.logPercentOver25WithBachelors = log(m.percentOver25WithBachelors)\n",
    "\"\"\"\n",
    "\n",
    "sparkConnector.write_cypher(msa_df_update_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Check that logs were committed to the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "msa_df_log_query = \"\"\"\n",
    "MATCH (m:MSA)\n",
    "RETURN m.name AS msa, \n",
    "m.population AS population,\n",
    "m.logPopulation,\n",
    "m.medianHouseholdIncome AS medianHouseholdIncome\n",
    "\"\"\"\n",
    "\n",
    "msa_df = sparkConnector.read_cypher(msa_df_log_query)\n",
    "\n",
    "msa_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Create in-memory graph projection\n",
    "Passing `\"*\"` as the third argument to `gds.graph.project` tells GDS to include any relationships that exist in the database in the in-memory graph. Because no relationships have been created in the graph yet, there will be no relationships in the in-memory graph projection when it is created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "graph_project_query = \"\"\"\n",
    "    CALL gds.graph.project(\n",
    "    'msa-graph', \n",
    "    'MSA', \n",
    "    '*', \n",
    "    {nodeProperties: [\"logPopulation\", \n",
    "        \"logMedianHouseholdIncome\", \n",
    "        \"logMedianHomePrice\", \n",
    "        \"logPercentOver25WithBachelors\"]})\n",
    "\"\"\"\n",
    "\n",
    "sparkConnector.write_script(graph_project_query)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Apply MinMax scalar to property values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "graph_scale_properties_mutations = \"\"\"\n",
    "CALL gds.alpha.scaleProperties.mutate(\"msa-graph\", {\n",
    "                                 nodeProperties: [\n",
    "                                     \"logPopulation\", \n",
    "                                     \"logMedianHouseholdIncome\", \n",
    "                                     \"logMedianHomePrice\", \n",
    "                                     \"logPercentOver25WithBachelors\"], \n",
    "                                 scaler : \"MinMax\",\n",
    "                                 mutateProperty : \"scaledProperties\"\n",
    "                                 })\n",
    "                                 \"\"\"\n",
    "\n",
    "sparkConnector.write_script(graph_scale_properties_mutations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "This next line streams node properties to the procedure caller."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "graph_stream_scaled_properties_query = \"\"\"\n",
    "CALL gds.graph.streamNodeProperty('msa-graph', 'scaledProperties')\n",
    "YIELD nodeId, propertyValue\n",
    "RETURN nodeId, propertyValue\n",
    "                                 \"\"\"\n",
    "sp_df = sparkConnector.read_cypher(graph_stream_scaled_properties_query)\n",
    "\n",
    "pandas_sp_df=sp_df.toPandas()\n",
    "pd.DataFrame(list(pandas_sp_df['propertyValue'])).iloc[:,0].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Cleanup resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(list(pandas_sp_df['propertyValue'])).iloc[:,1].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(list(pandas_sp_df['propertyValue'])).iloc[:,2].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(list(pandas_sp_df['propertyValue'])).iloc[:,3].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Run KNN to create relationships to nearest neighbors\n",
    "First run in stats mode and look at the similarity distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "knn_stats_query = \"\"\"CALL gds.knn.stats(\"msa-graph\",\n",
    "   {\n",
    "      nodeProperties:{\n",
    "      scaledProperties:\"EUCLIDEAN\"},\n",
    "      topK:15,\n",
    "      similarityCutoff: 0.8350143432617188,\n",
    "      sampleRate:1,\n",
    "      randomSeed:42,\n",
    "      concurrency:1\n",
    "   }\n",
    ") \n",
    "YIELD similarityDistribution \n",
    "RETURN similarityDistribution \"\"\"\n",
    "                                    \n",
    "knn_stats=sparkConnector.read_cypher(knn_stats_query)\n",
    "knn_stats.collect()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Write KNN nearest neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "knn_write = f\"\"\"CALL gds.knn.mutate(\"msa-graph\",\n",
    "               {{nodeProperties: {{scaledProperties: \"EUCLIDEAN\"}},\n",
    "               topK: 15,\n",
    "               mutateRelationshipType: \"IS_SIMILAR\",\n",
    "               mutateProperty: \"similarity\",\n",
    "               similarityCutoff: {knn_stats.collect()[0]['similarityDistribution']['p1']},\n",
    "               sampleRate:1,\n",
    "               randomSeed:42,\n",
    "               concurrency:1}}\n",
    "              ) \"\"\"\n",
    "\n",
    "#print(knn_write)\n",
    "sparkConnector.write_script(knn_write)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Write back to Neo4j graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "similarity_relationship_writeback = \"\"\"CALL gds.graph.writeRelationship(\n",
    "    \"msa-graph\",\n",
    "    \"IS_SIMILAR\",\n",
    "    \"similarity\"\n",
    ")\"\"\"\n",
    "\n",
    "sparkConnector.write_script(similarity_relationship_writeback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Add rank updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "add_rank_update = \"\"\"\n",
    "MATCH (m:MSA)-[s:IS_SIMILAR]->()\n",
    "WITH m, s ORDER BY s.similarity DESC\n",
    "WITH m, collect(s) as similarities, range(0, 11) AS ranks\n",
    "UNWIND ranks AS rank\n",
    "WITH rank, similarities[rank] AS rel\n",
    "SET rel.rank = rank + 1\n",
    "\"\"\"\n",
    "\n",
    "sparkConnector.write_cypher(add_rank_update)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Run Louvain Community Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "See how many communities Louvain is going to recommend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "read_louvain = \"\"\"\n",
    "CALL gds.louvain.stats('msa-graph',\n",
    "{relationshipTypes: [\"IS_SIMILAR\"],\n",
    "relationshipWeightProperty:\"similarity\"})\n",
    "YIELD communityCount, modularities\n",
    "RETURN communityCount, modularities\n",
    "\"\"\"\n",
    "sparkConnector.read_cypher(read_louvain).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now commit louvain communities to database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "write_louvain = \"\"\"\n",
    "CALL gds.louvain.write('msa-graph',\n",
    "{relationshipTypes: [\"IS_SIMILAR\"],\n",
    "relationshipWeightProperty:\"similarity\",\n",
    " writeProperty:\"communityId\"})\n",
    "YIELD communityCount, modularities\n",
    "RETURN communityCount, modularities\n",
    "\"\"\"\n",
    "\n",
    "sparkConnector.write_cypher(write_louvain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "community_query = \"\"\"\n",
    "MATCH (m:MSA)\n",
    "WITH m \n",
    "ORDER BY apoc.coll.sum([(m)-[s:IS_SIMILAR]->(m2) \n",
    "WHERE m.communityId = m2.communityId | s.similarity]) desc\n",
    "RETURN m.communityId as communityId,\n",
    "count(m) as msaCount, \n",
    "avg(m.population) as avgPopulation,\n",
    "avg(m.medianHomePrice) as avgHomePrice,\n",
    "avg(m.medianHouseholdIncome) as avgIncome,\n",
    "avg(m.percentOver25WithBachelors) as avgPctBachelors,\n",
    "collect(m.name)[..3] as exampleMSAs\n",
    "\"\"\"\n",
    "\n",
    "## Removed final sort because this doesn]t work with Spark \n",
    "## ORDER BY avgPopulation DESC\n",
    "## post sorting in spark\n",
    "                                      \n",
    "community_df=sparkConnector.read_cypher(community_query).sort(\"avgPopulation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pd_community_df=community_df.toPandas()\n",
    "pd_community_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(5, 1)\n",
    "fig.set_size_inches(6,20)\n",
    "for i in range(1,6):\n",
    "    sns.barplot(data=pd_community_df, x=\"communityId\", y=pd_community_df.columns[i], ax=axes[i-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Mean can give us a quick overview of properties, but can be skewed by outliers. Compare emperical cumulative distribution function (ECDF) at various proportions to get a more complete picture of distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# we need to remove sort by here\n",
    "detail_query=\"\"\"\n",
    "MATCH (m:MSA)\n",
    "RETURN \"community \" + m.communityId as communityId,\n",
    "m.population as population,\n",
    "m.medianHomePrice as medianHomePrice,\n",
    "m.medianHouseholdIncome as medianIncome,\n",
    "m.percentOver25WithBachelors as pctBachelors\n",
    "\"\"\"\n",
    "\n",
    "## post sorting in spark\n",
    "\n",
    "detail_df=sparkConnector.read_cypher(detail_query).sort(\"communityId\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pd_detail_df=detail_df.toPandas()\n",
    "pd_detail_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(4, 1)\n",
    "fig.set_size_inches(6,20)\n",
    "for i in range(1,5):\n",
    "    sns.ecdfplot(data=pd_detail_df, hue=\"communityId\", x=pd_detail_df.columns[i], log_scale=True, ax=axes[i-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Compare two-dimensions on scatter plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "splot = sns.scatterplot(data=pd_detail_df, x=\"medianIncome\", y=\"population\", hue=\"communityId\")\n",
    "splot.set(yscale=\"log\")\n",
    "splot.set(xscale=\"log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "splot = sns.scatterplot(data=pd_detail_df, x=\"pctBachelors\", y=\"medianHomePrice\", hue=\"communityId\")\n",
    "splot.set(yscale=\"log\")\n",
    "splot.set(xscale=\"log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Optional: assign human-friendly names to the clusters discovered.\n",
    "The Louvain community detection algorithm is not deterministic. You should have roughly the same clusters from previous runs, but some edge cases might be assigned to different communities. The community numbers might be shuffled between across different runs.  \n",
    "**This step requires adjustment by hand: choose from community IDs above.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "update_community_name_query=\"\"\"\n",
    "MATCH (m:MSA) \n",
    "  SET m.communityName = CASE m.communityId \n",
    "  WHEN 56 THEN \"Large mid-cost metros\"\n",
    "  WHEN 83 THEN \"College towns\"\n",
    "  WHEN 254 THEN \"Large high-cost metros\"\n",
    "  WHEN 266 THEN \"Mid-size metros\"\n",
    "  WHEN 277 THEN \"Small metros\"\n",
    "  WHEN 315 THEN \"Mid-price metros\"\n",
    "  WHEN 333 THEN \"Low-income metros\"\n",
    "  END\n",
    "\"\"\"\n",
    "\n",
    "sparkConnector.write_cypher(update_community_name_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Check on updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "describe_query=\"\"\"\n",
    "MATCH (m:MSA)\n",
    "return m.communityName, m.communityId, count(*)\n",
    "\"\"\"\n",
    "\n",
    "sparkConnector.read_cypher(describe_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Create an index on the communityName property to make it searchable in Bloom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sparkConnector.write_script(\"\"\"\n",
    "CREATE INDEX msa_community_name IF NOT EXISTS\n",
    "FOR (m:MSA) ON (m.communityName)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now open Bloom and do some additional analysis!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "graph_project_drop = \"\"\"\n",
    "    CALL gds.graph.drop(\n",
    "    'msa-graph')\n",
    "\"\"\"\n",
    "sparkConnector.write_script(graph_project_drop)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}