{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# BigQuery Read/Write with Neo4j Spark Connector and PySpark\n",
    "In the examples that follows, we will be using the Spark Connector running under PySpark\n",
    "[Neo4j spark connector under Python](https://neo4j.com/docs/spark/current/python/)\n",
    "Please run this notebook from a valid Spark environment.  It was tested under [DataProc](https://cloud.google.com/dataproc)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Setup Neo4j instance\n",
    "Create a free account at https://sandbox.neo4j.com. Choose the “Blank Sandbox - Graph Data Science” option.\n",
    "When your sandbox has been created, fill in the Bolt URL and password below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pip install --upgrade seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pip install --upgrade matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Setup Neo4j Spark Connector imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Define Neo4j connection variables.  Yours will be different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "neo4j_url = \"bolt://<ip>:7687\"\n",
    "neo4j_user = \"neo4j\"\n",
    "neo4j_password = \"<pwd>\"\n",
    "neo4j_database= \"neo4j\"\n",
    "tmp_working_bucket = \"neo4j-sandbox/dataproc-working\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Create Spark Session, seeded with Neo4j packages.  If you don't want to wait for the download each time, load the connector into the master node using SSH.\n",
    "\n",
    "Note: we are adding the Neo4j data warehouse connector and BigQuery library in the library packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "spark = (SparkSession.builder\n",
    "        .appName('Leverage Neo4j with Apache Spark')\n",
    "        .master('local[*]')\n",
    "        # Just to show dataframes as tables\n",
    "        .config('spark.sql.repl.eagerEval.enabled', True)\n",
    "        # On DataProc we must use spark 2.x\n",
    "        .config('spark.jars.packages', \"org.neo4j:neo4j-connector-apache-spark_2.12:4.1.3_for_spark_2.4,com.google.cloud.spark:spark-bigquery-with-dependencies_2.12:0.26.0\")\n",
    "        # These global credentials don't cascade on DataProc\n",
    "        .config('neo4j.url', neo4j_url)\n",
    "        .config('neo4j.authentication.type', \"basic\")\n",
    "        .config('neo4j.authentication.basic.username', neo4j_user)\n",
    "        .config('neo4j.authentication.basic.password', neo4j_password)\n",
    "        .getOrCreate())\n",
    "# output spark version\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Here we are going to create a utility class for the Spark Connector.  On DataProc the global configuration is not picked up so we need to supply credentials every time.  Also, some valid cypher statements will not run as cypher in the Spark Connector.  They must be executed as side effects through the \"script\" argument of a dummy write operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class SparkConnector:\n",
    "    \n",
    "    def __init__( self, spark_session, uri, user, pwd):\n",
    "        \n",
    "        self.__spark_session = spark_session\n",
    "        self.__uri = uri\n",
    "        self.__user = user\n",
    "        self.__pwd = pwd\n",
    "        self.__count_cypher = \"MATCH (n) RETURN count(n)\"\n",
    "        # initialize global dummy dataframe for write operations\n",
    "        self.__df = self.read_cypher(self.__count_cypher)\n",
    "            \n",
    "    # read is standard with credentials\n",
    "    def read_cypher(self, query):\n",
    "        \n",
    "        response = None\n",
    "        try: \n",
    "            response = self.__spark_session.read.format(\"org.neo4j.spark.DataSource\") \\\n",
    "                .option(\"query\", query) \\\n",
    "                .option(\"url\", self.__uri) \\\n",
    "                .option(\"authentication.type\", \"basic\") \\\n",
    "                .option(\"authentication.basic.username\", self.__user) \\\n",
    "                .option(\"authentication.basic.password\", self.__pwd) \\\n",
    "                .load()\n",
    "        except Exception as e:\n",
    "            print(\"Read query failed:\", e)\n",
    "\n",
    "        return response\n",
    "    \n",
    "    # write is standard with credentials\n",
    "    def write_cypher(self, query):\n",
    "        \n",
    "        response = None\n",
    "        try: \n",
    "            response = self.__df.write.format(\"org.neo4j.spark.DataSource\") \\\n",
    "                .option(\"query\", query) \\\n",
    "                .option(\"url\", self.__uri) \\\n",
    "                .option(\"authentication.type\", \"basic\") \\\n",
    "                .option(\"authentication.basic.username\", self.__user) \\\n",
    "                .option(\"authentication.basic.password\", self.__pwd) \\\n",
    "                .option(\"partitions\", \"1\") \\\n",
    "                .save()\n",
    "        except Exception as e:\n",
    "            print(\"Write query failed:\", e)\n",
    "\n",
    "        return response\n",
    "    \n",
    "    # write_script  is a work-around that using a no side-effects write query\n",
    "    def write_script(self,cypher):\n",
    "        \n",
    "        response = None\n",
    "        try: \n",
    "            response = self.__df.write.format(\"org.neo4j.spark.DataSource\") \\\n",
    "              .option(\"url\", self.__uri) \\\n",
    "              .option(\"authentication.type\", \"basic\") \\\n",
    "              .option(\"authentication.basic.username\", self.__user) \\\n",
    "              .option(\"authentication.basic.password\", self.__pwd) \\\n",
    "              .option(\"query\", \"MATCH (n) WHERE 1 = 2 SET n.placeholder = false\") \\\n",
    "              .option(\"script\",cypher) \\\n",
    "              .option(\"partitions\", \"1\") \\\n",
    "              .save()\n",
    "        except Exception as e:\n",
    "            print(\"Write query failed:\", e)\n",
    "\n",
    "        return response\n",
    "           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's now check that GDS is running on the server by executing this Cypher query.\n",
    "We only need to supply credentials once per notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sparkConnector = SparkConnector(spark_session=spark,uri=neo4j_url, user=neo4j_user, pwd=neo4j_password)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df=sparkConnector.read_cypher(\"return gds.version() as gds_version\")\n",
    "df.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sparkConnector.read_cypher(\"MATCH (n:MSA) RETURN count(n)\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Optional if database is not empty!  Reset it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "reset_db_query = \"\"\"CREATE OR REPLACE DATABASE `neo4j`\"\"\"\n",
    "sparkConnector.write_script(reset_db_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Check that it's empty now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sparkConnector.read_cypher(\"MATCH (n:MSA) RETURN count(n)\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Load MSA data from BigQuery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "This section leverages best practices described here: https://neo4j.com/docs/spark/current/dwh/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Create unique constraint on MSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sparkConnector.write_script(\"\"\"\n",
    "CREATE CONSTRAINT msa_name IF NOT EXISTS ON (m:MSA) ASSERT m.name IS NODE KEY\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "Create json template for reading and writing input from BigQuery. The data is here:\n",
    "\n",
    "    select name,\n",
    "       population,\n",
    "       medianHomePrice\n",
    "       percentOver25WithBachelors,\n",
    "       medianHouseholdIncome\n",
    "    from census.census_demos_by_msa"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Parallelized table scan\n",
    "The preferred approach for querying BigQuery is a simple table scan.\n",
    "This method supports maxParallelization if you want to throttle, otherwise it will maximize speed.\n",
    "\n",
    "https://github.com/GoogleCloudDataproc/spark-bigquery-connector/blob/master/README.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# If we're doing a table scan, the options table syntax can be used and provides automatic parallelism\n",
    "msa_table=\"\"\"\n",
    "    neo4jbusinessdev.census.msa_demos\n",
    "\"\"\"\n",
    "tableBqDf = spark.read.format(\"bigquery\").option(\"table\",msa_table).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tableBqDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Post query manipulation\n",
    "A common pattern is to create a temp view and then to use Spark SQL to requery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tableBqDf.createOrReplaceTempView(\"msa_df\")\n",
    "bigqueryPostQueryDf = spark.sql(\"SELECT * FROM msa_df WHERE population > 1000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "bigqueryPostQueryDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## SQL push-down, temporary materialization\n",
    "Another approach is to push down a full SQL statement to BigQuery\n",
    "This approach requires materialization of results in a BigQuery dataset.\n",
    "This dataset will be deleted after 1 day by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\"viewsEnabled\",\"true\")\n",
    "spark.conf.set(\"materializationDataset\",\"census\")\n",
    "# if this value is too low, big query will expect that it exists in cache.  Default is 1440.\n",
    "spark.conf.set(\"materializationExpirationTimeInMinutes\",5)\n",
    "\n",
    "msa_query=\"\"\"\n",
    "    SELECT name\n",
    "    ,population\n",
    "    ,percentOver25WithBachelors\n",
    "    ,medianHomePrice\n",
    "    ,medianHouseholdIncome\n",
    "    FROM `neo4jbusinessdev.census.msa_demos`\n",
    "\"\"\"\n",
    "bigqueryDf = spark.read.format(\"bigquery\").option(\"query\",msa_query).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "bigqueryDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Load data from BigQuery into Neo4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "(bigqueryDf.write\n",
    "  .format(\"org.neo4j.spark.DataSource\")\n",
    "  .mode(\"ErrorIfExists\")\n",
    "  .option(\"labels\", \":MSA\")\n",
    "  .option(\"url\", neo4j_url) \n",
    "  .option(\"authentication.type\", \"basic\") \n",
    "  .option(\"authentication.basic.username\", neo4j_user) \n",
    "  .option(\"authentication.basic.password\", neo4j_password) \n",
    "  .option(\"partitions\", \"1\") \n",
    "  .save())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Check for data now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sparkConnector.read_cypher(\"MATCH (n:MSA) RETURN count(n)\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "msa_df_query = \"\"\"\n",
    "MATCH (m:MSA)\n",
    "RETURN m.name AS msa, \n",
    "m.population AS population,\n",
    "m.medianHouseholdIncome AS medianHouseholdIncome,\n",
    "m.medianHomePrice AS medianHomePrice,\n",
    "m.percentOver25WithBachelors as percentOver25WithBachelors\"\"\"\n",
    "\n",
    "msa_df=sparkConnector.read_cypher(msa_df_query)\n",
    "msa_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Convert Spark dataframe to pandas to display histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pandas_msa_df=msa_df.toPandas()\n",
    "print(pandas_msa_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(4, 2)\n",
    "fig.set_size_inches(15,30)\n",
    "for i in range(1,5):\n",
    "    sns.histplot(pandas_msa_df.iloc[:,i], ax=axes[i-1,0])\n",
    "    sns.histplot(pandas_msa_df.iloc[:, i], log_scale=True, ax=axes[i-1,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "That log transformation looks like it will help. Run the Cypher to store the transformed values in the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "msa_df_update_query = \"\"\"\n",
    "MATCH (m:MSA)\n",
    "SET \n",
    "m.logPopulation = log(m.population),\n",
    "m.logMedianHouseholdIncome = log(m.medianHouseholdIncome),\n",
    "m.logMedianHomePrice = log(m.medianHomePrice),\n",
    "m.logPercentOver25WithBachelors = log(m.percentOver25WithBachelors)\n",
    "\"\"\"\n",
    "\n",
    "sparkConnector.write_cypher(msa_df_update_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Check that logs were committed to the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "msa_df_log_query = \"\"\"\n",
    "MATCH (m:MSA)\n",
    "RETURN m.name AS msa, \n",
    "m.population AS population,\n",
    "m.logPopulation,\n",
    "m.medianHouseholdIncome AS medianHouseholdIncome\n",
    "\"\"\"\n",
    "\n",
    "msa_df = sparkConnector.read_cypher(msa_df_log_query)\n",
    "\n",
    "msa_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Create in-memory graph projection\n",
    "Passing `\"*\"` as the third argument to `gds.graph.project` tells GDS to include any relationships that exist in the database in the in-memory graph. Because no relationships have been created in the graph yet, there will be no relationships in the in-memory graph projection when it is created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "graph_project_query = \"\"\"\n",
    "    CALL gds.graph.project(\n",
    "    'msa-graph', \n",
    "    'MSA', \n",
    "    '*', \n",
    "    {nodeProperties: [\"logPopulation\", \n",
    "        \"logMedianHouseholdIncome\", \n",
    "        \"logMedianHomePrice\", \n",
    "        \"logPercentOver25WithBachelors\"]})\n",
    "\"\"\"\n",
    "\n",
    "sparkConnector.write_script(graph_project_query)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Apply MinMax scalar to property values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "graph_scale_properties_mutations = \"\"\"\n",
    "CALL gds.alpha.scaleProperties.mutate(\"msa-graph\", {\n",
    "                                 nodeProperties: [\n",
    "                                     \"logPopulation\", \n",
    "                                     \"logMedianHouseholdIncome\", \n",
    "                                     \"logMedianHomePrice\", \n",
    "                                     \"logPercentOver25WithBachelors\"], \n",
    "                                 scaler : \"MinMax\",\n",
    "                                 mutateProperty : \"scaledProperties\"\n",
    "                                 })\n",
    "                                 \"\"\"\n",
    "\n",
    "sparkConnector.write_script(graph_scale_properties_mutations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "This next line streams node properties to the procedure caller."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "graph_stream_scaled_properties_query = \"\"\"\n",
    "CALL gds.graph.streamNodeProperty('msa-graph', 'scaledProperties')\n",
    "YIELD nodeId, propertyValue\n",
    "RETURN nodeId, propertyValue\n",
    "                                 \"\"\"\n",
    "sp_df = sparkConnector.read_cypher(graph_stream_scaled_properties_query)\n",
    "\n",
    "pandas_sp_df=sp_df.toPandas()\n",
    "pd.DataFrame(list(pandas_sp_df['propertyValue'])).iloc[:,0].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Cleanup resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(list(pandas_sp_df['propertyValue'])).iloc[:,1].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(list(pandas_sp_df['propertyValue'])).iloc[:,2].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(list(pandas_sp_df['propertyValue'])).iloc[:,3].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Run KNN to create relationships to nearest neighbors\n",
    "First run in stats mode and look at the similarity distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "knn_stats_query = \"\"\"CALL gds.knn.stats(\"msa-graph\",\n",
    "   {\n",
    "      nodeProperties:{\n",
    "      scaledProperties:\"EUCLIDEAN\"},\n",
    "      topK:15,\n",
    "      similarityCutoff: 0.8350143432617188,\n",
    "      sampleRate:1,\n",
    "      randomSeed:42,\n",
    "      concurrency:1\n",
    "   }\n",
    ") \n",
    "YIELD similarityDistribution \n",
    "RETURN similarityDistribution \"\"\"\n",
    "                                    \n",
    "knn_stats=sparkConnector.read_cypher(knn_stats_query)\n",
    "knn_stats.collect()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Write KNN nearest neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "knn_write = f\"\"\"CALL gds.knn.mutate(\"msa-graph\",\n",
    "               {{nodeProperties: {{scaledProperties: \"EUCLIDEAN\"}},\n",
    "               topK: 15,\n",
    "               mutateRelationshipType: \"IS_SIMILAR\",\n",
    "               mutateProperty: \"similarity\",\n",
    "               similarityCutoff: {knn_stats.collect()[0]['similarityDistribution']['p1']},\n",
    "               sampleRate:1,\n",
    "               randomSeed:42,\n",
    "               concurrency:1}}\n",
    "              ) \"\"\"\n",
    "\n",
    "#print(knn_write)\n",
    "sparkConnector.write_script(knn_write)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Write back to Neo4j graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "similarity_relationship_writeback = \"\"\"CALL gds.graph.writeRelationship(\n",
    "    \"msa-graph\",\n",
    "    \"IS_SIMILAR\",\n",
    "    \"similarity\"\n",
    ")\"\"\"\n",
    "\n",
    "sparkConnector.write_script(similarity_relationship_writeback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Add rank updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "add_rank_update = \"\"\"\n",
    "MATCH (m:MSA)-[s:IS_SIMILAR]->()\n",
    "WITH m, s ORDER BY s.similarity DESC\n",
    "WITH m, collect(s) as similarities, range(0, 11) AS ranks\n",
    "UNWIND ranks AS rank\n",
    "WITH rank, similarities[rank] AS rel\n",
    "SET rel.rank = rank + 1\n",
    "\"\"\"\n",
    "\n",
    "sparkConnector.write_cypher(add_rank_update)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Run Louvain Community Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "See how many communities Louvain is going to recommend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "read_louvain = \"\"\"\n",
    "CALL gds.louvain.stats('msa-graph',\n",
    "{relationshipTypes: [\"IS_SIMILAR\"],\n",
    "relationshipWeightProperty:\"similarity\"})\n",
    "YIELD communityCount, modularities\n",
    "RETURN communityCount, modularities\n",
    "\"\"\"\n",
    "sparkConnector.read_cypher(read_louvain).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now commit louvain communities to database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "write_louvain = \"\"\"\n",
    "CALL gds.louvain.write('msa-graph',\n",
    "{relationshipTypes: [\"IS_SIMILAR\"],\n",
    "relationshipWeightProperty:\"similarity\",\n",
    " writeProperty:\"communityId\"})\n",
    "YIELD communityCount, modularities\n",
    "RETURN communityCount, modularities\n",
    "\"\"\"\n",
    "\n",
    "sparkConnector.write_cypher(write_louvain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "community_query = \"\"\"\n",
    "MATCH (m:MSA)\n",
    "WITH m \n",
    "ORDER BY apoc.coll.sum([(m)-[s:IS_SIMILAR]->(m2) \n",
    "WHERE m.communityId = m2.communityId | s.similarity]) desc\n",
    "RETURN m.communityId as communityId,\n",
    "count(m) as msaCount, \n",
    "avg(m.population) as avgPopulation,\n",
    "avg(m.medianHomePrice) as avgHomePrice,\n",
    "avg(m.medianHouseholdIncome) as avgIncome,\n",
    "avg(m.percentOver25WithBachelors) as avgPctBachelors,\n",
    "collect(m.name)[..3] as exampleMSAs\n",
    "\"\"\"\n",
    "\n",
    "## Removed final sort because this doesn]t work with Spark \n",
    "## ORDER BY avgPopulation DESC\n",
    "## post sorting in spark\n",
    "                                      \n",
    "community_df=sparkConnector.read_cypher(community_query).sort(\"avgPopulation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pd_community_df=community_df.toPandas()\n",
    "pd_community_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(5, 1)\n",
    "fig.set_size_inches(6,20)\n",
    "for i in range(1,6):\n",
    "    sns.barplot(data=pd_community_df, x=\"communityId\", y=pd_community_df.columns[i], ax=axes[i-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Mean can give us a quick overview of properties, but can be skewed by outliers. Compare emperical cumulative distribution function (ECDF) at various proportions to get a more complete picture of distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# we need to remove sort by here\n",
    "detail_query=\"\"\"\n",
    "MATCH (m:MSA)\n",
    "RETURN \"community \" + m.communityId as communityId,\n",
    "m.population as population,\n",
    "m.medianHomePrice as medianHomePrice,\n",
    "m.medianHouseholdIncome as medianIncome,\n",
    "m.percentOver25WithBachelors as pctBachelors\n",
    "\"\"\"\n",
    "\n",
    "## post sorting in spark\n",
    "\n",
    "detail_df=sparkConnector.read_cypher(detail_query).sort(\"communityId\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pd_detail_df=detail_df.toPandas()\n",
    "pd_detail_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(4, 1)\n",
    "fig.set_size_inches(6,20)\n",
    "for i in range(1,5):\n",
    "    sns.ecdfplot(data=pd_detail_df, hue=\"communityId\", x=pd_detail_df.columns[i], log_scale=True, ax=axes[i-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Compare two-dimensions on scatter plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "splot = sns.scatterplot(data=pd_detail_df, x=\"medianIncome\", y=\"population\", hue=\"communityId\")\n",
    "splot.set(yscale=\"log\")\n",
    "splot.set(xscale=\"log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "splot = sns.scatterplot(data=pd_detail_df, x=\"pctBachelors\", y=\"medianHomePrice\", hue=\"communityId\")\n",
    "splot.set(yscale=\"log\")\n",
    "splot.set(xscale=\"log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Write enriched features back to BigQuery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In this section, we are going to write features back to BigQuery.  Let's begin by previewing a query that we will materialize to BigQuery.  Note that you can use SKIP and LIMIT in cypher when using the Spark connector since it is batching the query internally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "relationship_cypher=\"\"\"\n",
    "MATCH (s)-[r:IS_SIMILAR]-(t) \n",
    "RETURN s.name AS msa_name\n",
    ",t.name AS related_name\n",
    ",r.similarity AS similarity\n",
    ",s.communityId AS louvain_community_id\n",
    "\"\"\"\n",
    "relsDf=sparkConnector.read_cypher(relationship_cypher)\n",
    "relsDf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now write to bigquery.  Note that writes require a temporary storage location for Avro files in process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# tmp_working_bucket\n",
    "spark.conf.set(\"temporaryGcsBucket\", \"neo4j_sandbox\")\n",
    "(relsDf.write.format(\"bigquery\")\n",
    "  .mode(\"overwrite\") \n",
    "  .option(\"table\",\"neo4jbusinessdev.census.msa_demo_similarity\")\n",
    "  .save())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now look for data in BigQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "spark.read.format(\"bigquery\").option(\"table\",\"census.msa_demo_similarity\").load().show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Optional: assign human-friendly names to the clusters discovered.\n",
    "The Louvain community detection algorithm is not deterministic. You should have roughly the same clusters from previous runs, but some edge cases might be assigned to different communities. The community numbers might be shuffled between across different runs.  \n",
    "**This step requires adjustment by hand: choose from community IDs above.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "update_community_name_query=\"\"\"\n",
    "MATCH (m:MSA) \n",
    "  SET m.communityName = CASE m.communityId \n",
    "  WHEN 56 THEN \"Large mid-cost metros\"\n",
    "  WHEN 83 THEN \"College towns\"\n",
    "  WHEN 254 THEN \"Large high-cost metros\"\n",
    "  WHEN 266 THEN \"Mid-size metros\"\n",
    "  WHEN 277 THEN \"Small metros\"\n",
    "  WHEN 315 THEN \"Mid-price metros\"\n",
    "  WHEN 333 THEN \"Low-income metros\"\n",
    "  END\n",
    "\"\"\"\n",
    "\n",
    "sparkConnector.write_cypher(update_community_name_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Check on updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "describe_query=\"\"\"\n",
    "MATCH (m:MSA)\n",
    "return m.communityName, m.communityId, count(*)\n",
    "\"\"\"\n",
    "\n",
    "sparkConnector.read_cypher(describe_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Create an index on the communityName property to make it searchable in Bloom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sparkConnector.write_script(\"\"\"\n",
    "CREATE INDEX msa_community_name IF NOT EXISTS\n",
    "FOR (m:MSA) ON (m.communityName)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now open Bloom and do some additional analysis!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Drop graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "graph_project_drop = \"\"\"\n",
    "    CALL gds.graph.drop(\n",
    "    'msa-graph')\n",
    "\"\"\"\n",
    "sparkConnector.write_script(graph_project_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Drop BigQuery table.  There is no native commands to delete with spark so let's use the Python API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "\n",
    "# Construct a BigQuery client object.\n",
    "client = bigquery.Client()\n",
    "table_id = 'neo4jbusinessdev.census.msa_demo_similarity'\n",
    "# Will raisegoogle.api_core.exceptions.NotFound unless not_found_ok is True.\n",
    "client.delete_table(table_id, not_found_ok=True)  # Make an API request.\n",
    "print(\"Deleted table '{}'.\".format(table_id))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We can verify deletion here...  Error is expected!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "spark.read.format(\"bigquery\").option(\"table\",\"census.msa_demo_similarity\").load().show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}